# -*- coding: utf-8 -*-
"""CAPSTONE-3-BIKE-SHARING-REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1piw_zwtrRP-vtOViZpoBK-RBgcXPalfC

# **Bike Sharing Demand Prediction**

**Table of content :**

1. Business Problem Understanding
2. Data Understanding
3. Exploratory Data Analysis
4. Data Preparation
5. Modeling
6. Conclusion and Recommendation
7. Deployment

****

# **Business Understanding**

# Introduction

Bike-sharing systems are a new generation of traditional bike rentals where the whole process, from membership, rental, and return back, has become automatic. Through these systems, a user can easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousand bicycles. Today, great interest exists in these systems due to their important role in traffic, environmental, and health issues.


Apart from interesting real-world applications of bike-sharing systems, the characteristics of data generated by these systems make them attractive for research. Unlike other transport services such as buses or subways, the duration of travel, departure, and arrival position is explicitly recorded in these systems. This feature turns the bike-sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that the most important events in the city could be detected by monitoring these data.

# Problem and Objective

The objective of this Case is to Predication of bike rental count on daily based on the environmental and seasonal settings.

# **Data Understanding**

<!-- * Jelaskan asal sumber data dan kapan data dibuat.
* Jelaskan data apa yang dimiliki, dan deskripsikan setiap kolom & baris nya merepresentasikan apa.
* Jelaskan bagaimana kaitan antara atribut/kolom dengan konteks bisnis.
* Jelaskan tipe variabel dari tiap atribut.
* Jelaskan value apa saja yang anda temui didalam dataset.
* Berikan statement apakah data anda telah capable untuk digunakan menjawab problem atau belum. -->

The obtained data consists of a two-year historical log corresponding to the years 2011 and 2012 from the Capital Bikeshare system in Washington D.C., USA. This data is publicly available at http://capitalbikeshare.com/system-data and includes hourly records with relevant weather and seasonal information, sourced from http://www.freemeteo.com.

**`FEATURES & TARGET`**

Short description of features and targets

| Features | Description |
|-|-|
| `dteday` | Rental date |
| `season` | Season (1: Winter, 2: Spring, 3: Summer, 4: Fall) |
| `hr` | Hour of day (0 to 23) |
| `holiday` | Holiday or not |
| `temp` | Normalized temperature in Celsius using MinMaxScaler (min=-8, max=39) |
| `atemp` | Normalized feeling temperature in Celsius using MinMaxScaler (min=-16, max=50) |
| `hum` | Normalized humidity (divided by 100) |
| `weathersit` | Weather situation |
| | 1: Clear, Few clouds, Partly cloudy, Partly cloudy |
| | 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist |
| | 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds |
| | 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog |
| `casual` | Count of bikes rented to casual users |
| `registered` | Count of bikes rented to registered users |

====================================================================================  

| Target | Description |
|-|-|
| `cnt` | Total number of bikes rented, including both casual and registered users |

**Load Libraries**
"""

# pip install pycaret

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

pd.set_option('display.max_columns', None)

color = sns.color_palette("tab10")
sns.set_style('darkgrid')

"""**Load Dataset**"""

# data = pd.read_csv("../data/data_bike_sharing.csv")
data = pd.read_csv("/content/data_bike_sharing.csv")
print('Dataset Shape:',data.shape)
display(data.head())

pd.DataFrame({
    'feature': data.columns.values,  # Column names
    'data_type': data.dtypes.values,  # Data types of columns
    'null_value(%)': data.isna().mean().values * 100,  # Percentage of null values in each column
    'neg_value(%)': [len(data[col][data[col] < 0]) / len(data) * 100 if col in data.select_dtypes(include=[np.number]).columns else 0 for col in data.columns],  # Percentage of negative values in numeric columns
    '0_value(%)': [len(data[col][data[col] == 0]) / len(data) * 100 if col in data.select_dtypes(include=[np.number]).columns else 0 for col in data.columns],  # Percentage of zero values in numeric columns
    'duplicate': data.duplicated().sum(),  # Number of duplicate rows
    'n_unique': data.nunique().values,  # Number of unique values in each column
    'sample_unique': [data[col].unique() for col in data.columns]  # Unique values in each column
}).round(3)

"""`Insight`
- There are no missing, negative, or duplicate values in the dataset.

- Some features have values equal to 0.

- Some features have values ​​that have been normalized.

- The 'dteday' column is of datetime type.

- Except for one column, all other columns are of float or integer type.

- Some categorical fields are represented as integers, such as 'holiday', 'season', 'weathersit'.

Overall, the provided features meet the criteria for accurately illustrating the factors influencing bike sharing demand.

# **Exploratory Data Analysis (EDA)**

<!-- * Bagaimana bentuk distribusi data?
* Bagaimana korelasi data pada dataset?
* Bagaimana keterkaitan antara feature dengan target? Bisa lakukan analisis bivariate. -->

## Data Distribution
"""

fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(15, 6))
axs = axs.flatten()

for i in range(1, len(data.columns)):
    sns.boxplot(x=data.columns[i], data=data, ax=axs[i-1], orient='v')

plt.suptitle('Distribution of Different Features', y=1.02, fontsize=16)
plt.tight_layout()
plt.show()

"""`Insight`

- Based on the boxplots, some features like 'hr', 'season', 'atemp', and 'temp' visually display a normal distribution.

- Apart from that, the majority of features exhibit a positive or right-skewed distribution with some outliers.

- Furthermore, outliers are noticeable in several features, including 'weathersit', 'hum', 'casual', 'registered', and 'cnt'.

**The Shapiro Normality Test**
"""

from scipy.stats import shapiro

dist = []

for i in range(1, len(data.columns)):
    feature_name = data.columns[i]
    stat, p_value = shapiro(data[feature_name])

    if p_value > 0.05:
        distribution = "Normally Distributed"
    else:
        distribution = "Not Normally Distributed"

    dist.append([feature_name, stat, p_value, distribution])

dist_df = pd.DataFrame(dist, columns=['Feature', 'Shapiro-Wilk Statistic', 'P-value', 'Distributed'])
dist_df

"""`Insight`

The Shapiro-Wilk normality test indicates that `none of the features exhibit a normal distribution`.

## Univariate Analysis

**Categorical Features**

Create dummy variables for visualization
"""

data['dteday'] = pd.to_datetime(data['dteday'])

data['weekday'] = data['dteday'].dt.weekday
data['day'] = data['dteday'].dt.day
data['month'] = data['dteday'].dt.month
data['year'] = data['dteday'].dt.year
data.head()

"""| Features | Description |
|-|-|
| `hr` | Hour (0 s/d 23) |
| `weekday` | Day of week |

**Visualization of Each Feature**
"""

fig, axs = plt.subplots(1, 2, figsize=(15,4))
sns.set_style('darkgrid')

max_pallete = ['#B4B4B3' if h != 17 else color[0] for h in data['hr'].sort_values().unique()]

sns.boxplot(data=data, x='hr', y='cnt', ax=axs[0], palette=max_pallete)
sns.boxplot(data=data, x='weekday', y='cnt', ax=axs[1], palette=color)

weekday = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
axs[0].set_xlabel('hour')
axs[1].set_xticklabels(weekday)
axs[1].set_xlabel('day of week')

plt.suptitle('Bike Sharing Count Average by Hour and Day of Week', fontsize=16)
plt.tight_layout()
plt.show()

"""`Insight`

- Bike sharing start increasing from 6 AM and peak at 8 AM, reflecting a considerable number of people using bikes for commuting to work or school.

- The biggest increase occurred at 5 PM, possibly indicating people were returning from work.

- From midnight (0 AM) to early morning (4 AM), bike sharing are low, reflecting minimal activity during these hours.

- Bike sharing median show an increase on weekdays (Monday to Friday) and a decrease on weekends (Saturday and Sunday) but are not significant.

- There is a relatively high consistency in bike sharing on weekdays, indicating a similar bike usage routine for people on workdays.

| Features | Description |
|-|-|
| `month` | Month of year |
| `season` | Seasonal (1: Winter, 2: Spring, 3: Summer, 4: Fall) |
| `year` | Year |
"""

import seaborn as sns
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 2, figsize=(15, 4))
max_palette = ['#B4B4B3' if m != 9 else color[0] for m in data['dteday'].dt.month.sort_values().unique()]

sns.boxplot(data=data, x='month', y='cnt', ax=axs[0], palette=max_palette)
sns.boxplot(data=data, x='year', y='cnt', ax=axs[1], palette=color)

month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

axs[0].set_xlabel('Month')
axs[0].set_xticklabels(month_order)
axs[1].set_xlabel('Year')

plt.suptitle('Bike Sharing Count Average by Month and Year', fontsize=16)
plt.tight_layout()
plt.show()

"""`Insight`

- Median bike sharing count start increasing in March and peak in September, reflecting a increase in bike usage during the summer and late summer seasons.

- Bike sharing significantly decrease during the winter months, especially from December to February, aligning with the expected decline due to colder weather.

- Apart from summer, bike sharing also show an increase during the spring months (March and April) and decrease during the fall months (October and November).

- The bike usage pattern underscores the significance of seasons in influencing people's decisions to bike.

- There is a significance increase in bike sharing from the year 2011 to 2012, indicating growth in the popularity of the bike-sharing system during that period.
"""

fig, axs = plt.subplots(1, 3, figsize=(15,4))

sns.boxplot(data=data, x='holiday', y='cnt', ax=axs[0], palette=color)
sns.boxplot(data=data, x='weathersit', y='cnt', ax=axs[1], palette=color)
sns.boxplot(data=data, x='season', y='cnt', ax=axs[2], palette=color)


plt.suptitle('Impact of Several Conditions on Bike Sharing Count Average', fontsize=16)
plt.tight_layout()
plt.show()

"""| Features | Description |
|-|-|
| `weathersit` | Weather condition |
| | 1: Clear, Few clouds, Partly cloudy, Partly cloudy |
| | 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist |
| | 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds |
| | 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog |
| `holiday` | Whether it's a holiday or not |
| `season` | Season (1: Winter, 2: Spring, 3: Summer, 4: Fall) |

`Insight`

- Weather conditions greatly affect bicycle rental. Better weather conditions (code 1) correspond to the highest number of bicycle rentals.

- Poorer weather conditions, such as mist or cloudy weather (code 2), result in lower bike sharing.

- Extreme weather conditions, like heavy rain, ice pellets, thunderstorms, and thick fog (codes 3 and 4), have a significant impact on reducing bike sharing.

- This indicates that good and clear weather encourages people to use bikes more. This insight underscores the importance of weather analysis in the operational planning of bike-sharing systems.

- The number of bike sharing is lower on holidays (code 1) compared to regular days (code 0).

- Seasonal variations influence bike sharing. Summer (code 3) has the highest bike sharing, followed by fall (code 4), spring (code 2), and winter (code 1) with the lowest bike sharing.

**Numerical Features**

| Features | Description |
|-|-|
| `temp` | Normalized temperature in Celsius using MinMaxScaler (min=-8, max=39) |
| `atemp` | Normalized feeling temperature in Celsius using MinMaxScaler (min=-16, max=50) |
| `hum` | Normalized humidity (divided by 100) |
| `casual` | Count of bikes rented to casual users |
| `registered` | Count of bikes rented to registered users |

Convert to actual value to visualize data.
"""

temp_min, temp_max = -8, 39
atemp_min, atemp_max = -16, 50

original_atemp = data['atemp']*(atemp_max-atemp_min)+atemp_min
original_temp = data['temp']*(temp_max-temp_min)+temp_min
original_hum = data['hum']*100

fig, axs = plt.subplots(2, 2, figsize=(15,8))
axs.flatten()

sns.scatterplot(data=data, x=original_atemp, y='cnt', ax=axs[0,0])
sns.regplot(data=data, x=original_atemp, y='cnt', ax=axs[0, 0], scatter=False, color=color[3])

sns.scatterplot(data=data, x=original_temp, y='cnt', ax=axs[0,1])
sns.regplot(data=data, x=original_temp, y='cnt', ax=axs[0, 1], scatter=False, color=color[3])

sns.scatterplot(data=data, x=original_hum, y='cnt', ax=axs[1,0])
sns.regplot(data=data, x=original_hum, y='cnt', ax=axs[1, 0], scatter=False, color=color[3])

plt.suptitle('Scatter Plots of Numerical Features', fontsize=16)
plt.tight_layout()
plt.show()

"""`Insight`

- The relationship 'temp' or 'atemp' features to 'cnt' has a positive linear pattern.

- Value 0 in 'hum' feature looks like an outlier because it is far from dominant.

- There is a negative linear relationship between 'hum' and 'cnt', where when the humidity level increases, the number of bicycle uses tends to decrease.

## Bivariate Analysis

**Hour and Weekday on Count**
"""

fig, ax = plt.subplots(figsize=(20,8))
sns.pointplot(data=data, x='hr', y='cnt', hue='weekday', palette=color, ax=ax)
plt.title('Count of bikes during weekdays and weekends', fontsize=16)
plt.show()

"""`Insight`

- A significant increase in bike sharing users during the hours of 6 - 8 am only occurs on weekdays Monday - Friday (Code 0 - 4) which indicates that people use bicycles to go to school or work.

- Likewise, at 16 - 18 pm there is a significant increase on weekdays (Monday - Friday) which is the time back from work.

- On weekends Saturday and Sunday (Code 5 and 6) there tends to be an increase during the day until it peaks at 13 noon.

**Hour and Weekday on Casual and Registered User**
"""

fig, axs = plt.subplots(2, 1, figsize=(15,12))
sns.pointplot(data=data, x='hr', y='casual', hue='weekday', palette=color, ax=axs[0])
sns.pointplot(data=data, x='hr', y='registered', hue='weekday', palette=color, ax=axs[1])

axs[0].set_title('Count of bikes during weekdays and weekends: Casual Users', fontsize=14)
axs[1].set_title('Count of bikes during weekdays and weekends: Registered Users', fontsize=14)

plt.show()

"""`Insight`

- Casual users use bicycles more on weekends (Saturday and Sunday) in the midday to afternoon.

- Registered users mostly use bicycles on weekdays (Monday - Friday) in the morning and evening, which indicates that users use bicycles for work.

## Data Correlation
"""

plt.figure(figsize=(14,7))
sns.heatmap(data.corr('spearman'), annot=True, vmin=-1, vmax=1, cmap='coolwarm', annot_kws={'size': 10})


plt.title('Spearman Correlation Heatmap', fontsize=16)
plt.show()

"""`Insight`

- The `'cnt'` feature has a very high correlation with the `'casual'` feature and the `'registered'` feature. This is because the total bike sharing recorded in 'cnt' comes from these two user groups. The two features, both 'casual' and 'registered', will be dropped because these two features are actually part of the target variable 'cnt' so they will cause redundancy.

- The feature `'month'` shows a high correlation with the feature `'season'` (0.83), which could potentially cause multicollinearity, but with a threshold of 0.9 for multicollinearity, this features pass.

- The variables `'temp'` (temperature) and `'atemp'` (feel-like temperature) show a very strong positive correlation (0.9), indicating a close relationship between them. Changes in 'temp' tend to be followed by changes in 'atemp', indicating potential multicollinearity. To address this issue, one of the features will be dropped.

- There is a moderate positive correlation between `'hr'` (hour of the day) and `'cnt'` (bike sharing count). This indicates that the bike sharing count is significantly influenced by the time of day.

- The features `'temp'` and `'atemp'` have a moderate positive correlation with the total number of bicycle rentals `'cnt'`. This shows that temperature influences the number of bicycle rentals. For example, bicycle rentals may increase in a relatively high temperature range and decrease in a relatively low temperature range.

- `'hum'` (humidity) shows a moderate negative correlation with `'cnt'`. This implies that lower humidity levels are associated with increased bike sharing. It's possible that people may prefer biking when the air is less humid.

- The features `'year'`, `'season'` and `'month'` show a low correlation with `'cnt'`. This indicates that the bike sharing count may be influenced by particular year, months or seasons.

- There is a low negative correlation between `'weathersit'` (weather situation) and `'cnt'`. This indicates that worse weather conditions (higher weather condition codes) may have a negative impact on the bike sharing count.

- The features `'holiday'`, `'weekday'`, and `'day'` have a very low correlation with `'cnt'`, indicating that the daydate, day of the week and whether it is a holiday or not do not have a significant impact on the bike sharing count.

# **Data Preparation**

<!-- * Identifikasi dan kuantifikasi masalah yang ada dalam data seperti:
    1. Missing value
    2. Duplicated value
    3. Outlier
    4. Type data
    5. Column name
    6. Rare label
    7. Cardinality
    8. Collinearity
* Lakukan penanganan terhadap masalah data yang ditemukan.
* Lakukan feature engineering seperti:
    1. Encoding
    2. Scaling
    3. Feature creation
    4. Feature selection
* Buatlah pipeline dari tahap ini. -->

**Split Data Training and Testing**
"""

from sklearn.model_selection import train_test_split

# data= pd.read_csv("../data/data_bike_sharing.csv")
data = pd.read_csv("/content/data_bike_sharing.csv")
data['dteday'] = pd.to_datetime(data['dteday'])


df_seen, df_unseen = train_test_split(data, test_size=0.2, random_state=10)

"""## Identify Problem in the Dataset.

**Missing Value, Duplicate, and Negative Value**
"""

pd.DataFrame({
    'feature': data.columns.values,
    'data_type': data.dtypes.values,
    'missing_value(%)': data.isna().mean().values * 100,
    'duplicate' : data.duplicated().sum(),
    'neg_value(%)': [len(data[col][data[col] < 0]) / len(data) * 100 if col in data.select_dtypes(include=[np.number]).columns else 0 for col in data.columns],
    'n_unique': data.nunique().values}
).round(3)

"""`Insight`

- There are no missing, negative, or duplicate values in the dataset.

- The 'dteday' column is of datetime type. We can extract the date and time.

- Except for one column, all other columns are of float or integer type.

- Some categorical fields are represented as integers, such as 'holiday', 'season', 'weathersit'.

**Identifying Outlier**
"""

def calculate_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

outlier_data = []

for column in data.columns:
    if column in ['dteday', 'holiday']:
        continue
    outlier_result, lower_bound, upper_bound = calculate_outliers(data, column)
    total_outlier = len(outlier_result)
    outlier_percentage = round(total_outlier / len(data[column]) * 100, 2)
    outlier_data.append([column, total_outlier, outlier_percentage, lower_bound, upper_bound])

outlier_df = pd.DataFrame(outlier_data, columns=["Column", "Total Outliers", "Percentage (%)", "Lower Bound", "Upper Bound"])
outlier_df

"""`Insight`

- Features `'casual'` and `'registered'` show a number of significant outliers but this feature will be dropped because these features are part of the target variable which can result in redundancy.

- The target variable `'cnt'` also has significant outliers, to overcome this we will apply a logarithmic function which will also make the distribution close to normal.

**The image below is an illustration of the data distribution before and after handling outliers.**
"""

Q1 = data['cnt'].quantile(0.25)
Q3 = data['cnt'].quantile(0.75)
IQR = Q3 - Q1

# Calculate bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data= data[(data['cnt'] >= lower_bound) & (data['cnt'] <= upper_bound)]
data['cnt']= np.log1p(data['cnt'])

from scipy.stats.mstats import winsorize

fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 6))

cols = ['weathersit', 'hum', 'cnt']

for i in range(len(cols)):
    sns.boxplot(x=data[cols[i]], data=data, ax=axs[i,0], orient='v')
    if i < 3:
        sns.boxplot(x=winsorize(data[cols[i]], limits=(0.01, 0.01)), data=data, ax=axs[i,1], orient='v')
        axs[i, 1].set_xlabel(cols[i])
    else:
        sns.boxplot(x=np.log1p(data[cols[i]]), data=data, ax=axs[i,1], orient='v')

plt.suptitle('Before and After Outlier Handling', y=1.02, fontsize=16)
plt.tight_layout()
plt.show()

"""`Insight`

Overall outliers will be significantly reduced and will not eliminate the outlier effect.

## Handling the Problem

| Probelm | Feature | Action |
|-|-|-|
| Missing Value | None | None |
| Duplicated Value | None | None |
| Outlier | 'weathersit' | Winsorize with upper and lower bound of 1% |
| | 'hum' | Winsorize with upper and lower bound of 1% |
| | 'cnt' | Logarithmic Function (on Modeling) |
| Multicollinearity | 'atemp' | Drop the column because it can result in multicollinearity |
| Redudancy | 'casual', 'registered' | Drop the column because these features are part of the target variable which can result in redundancy.|
"""

# pip install pycaret

from scipy.stats.mstats import winsorize
from pycaret.internal.preprocess.transformers import TransformerWrapper
from typing import Literal
from sklearn.base import BaseEstimator, TransformerMixin, OneToOneFeatureMixin

class HandlingOutliers(BaseEstimator, TransformerMixin, OneToOneFeatureMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):

        X['weathersit'] = winsorize(X['weathersit'], limits=(0.01, 0.01))
        X['hum'] = winsorize(X['hum'], limits=(0.01, 0.01))

        return X

    def set_output(self, transform: Literal['default', 'pandas']):
        return super().set_output(transform=transform)

"""## **Feature Engineering**

**Make Pipeline**
"""

from sklearn.preprocessing import OneHotEncoder
from pycaret.internal.preprocess.transformers import ExtractDateTimeFeatures
from sklearn.compose import ColumnTransformer
from mlxtend.feature_selection import ColumnSelector

outliers_handling = TransformerWrapper(
                    include=['weathersit', 'hum'],
                    transformer=HandlingOutliers())

extract_date = TransformerWrapper(
                include=['dteday'],
                transformer=ExtractDateTimeFeatures(
                    features=['day', 'month', 'year', 'weekday']))

selection = ColumnSelector(cols=['dteday_day', 'dteday_month', 'dteday_year', 'dteday_weekday', 'hr', 'holiday', 'season', 'weathersit', 'hum', 'temp'])

encoder = TransformerWrapper(
        include=['hr', 'dteday_day', 'dteday_month', 'dteday_year', 'dteday_weekday', 'season', 'weathersit'],
        transformer=OneHotEncoder(drop='first', handle_unknown='ignore'))

base_pipe = [
    ('extract date', extract_date),
    ('selection', selection),
    ('outlier handling', outliers_handling)
]

"""**What is used in the pipeline above:**

| Technique | Feature | Description |
|-|-|-|
| Winsorize | 'weathersit', 'hum', 'windspeed' | Winsorize is used to overcome outlier problems. This technique replaces outlier values ​​with previously determined upper bound or lower bound values. In this way, the data distribution becomes more controllable and less influenced by extreme values. |
| Feature Creation | 'dteday' | Feature creation is the process of creating new features from existing data, aimed at improving the quality of information in the dataset to support better analysis or predictions. Create new features such as day, date, month, year, by extracting from the 'dteday' feature. |
| Feature Drop | 'atemp', 'casual', 'registered | Feature selection by eliminating features that can cause multicollinearity and redundancy which can negatively affect model performance. |
| One-Hot Encoder | 'hr', 'dteday_day', 'dteday_month', 'dteday_year', 'dteday_weekday', 'season', 'weathersit' | One-Hot Encoder is a technique that converts categorical variables into binary vectors, allowing machine learning models to process categorical information effectively. This technique will be used in several distance-based models such as linear regression, ridge, lasso, SVR. |

# **Modeling Experiment**

<!-- * Atur environment untuk proses developing model ML, termasuk integrasi dengan pipeline data proprocessing.
* Lakukan proses berulang dari tahapan Benchmarking Model -> Hyperparameter Tunning -> Evaluating Model sampai diperoleh model terbaik. Anda bisa membuat banyak skenario eksperimen, misal:
    1. Perbandingan tanpa resampling vs dengan resampling
    2. Perbandingan antar set feature yang berbeda
    3. Perbandingan stand alone vs ensemble model
* Bandingkan performa antar kandidat model dari tiap skenario eksperimen yang telah dilakukan. Pilih kandidat terbaik sebagai final model.
* Lakukan evaluasi final model seperti:
    1. Learning curve (Overfitting atau underfitting)
    2. Residual analysis (Regression)
    3. ROC, PR Curve, Discriminant Threshold (Classification)
    4. Recursive Feature Elimination
    5. Time complexity
* Jawablah beberapa poin berikut:
    1. Bagaimana cara kerja final model?
    2. Apa limitasi dari final model?
    3. Kondisi data seperti apa kinerja model dapat dipercaya dan tidak dipercaya?
    4. Bagaimana interpretasi final model (feature importance, dll)?
* Gunakan final model untuk prediksi validation sets, dan berikan penjelasan.
* Lakukan proses finalisasi model, yaitu fitting ulang final model dengan seen data (training + validation set).
* Gunakan model untuk prediksi unseen data, dan berikan penjelasan.
* Lakukan analisis lanjutan dari hasil prediksi unseen data, misal analisis keuntungan, analisis kerugian, dll.  -->
"""

pip install catboost

from sklearn.compose import TransformedTargetRegressor
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.svm import SVR
from lightgbm import LGBMRegressor
from sklearn.linear_model import BayesianRidge

from pycaret.regression import *

"""**Setup Pycaret**"""

RANDOM_SEED = 42
K_FOLDS = 5

configs = {
    'data': df_seen,
    'train_size':  0.8,
    'preprocess':  False,
    'session_id':  RANDOM_SEED,
    'fold':  K_FOLDS,
    'custom_pipeline': base_pipe,
    'n_jobs':  -1,
}

setup(
    target = 'cnt',
    **configs
    )

get_config('pipeline')

"""The setup above is a setting for experiments using pycaret. The following is an explanation for each parameter used:

| Parameter | Description |
|-|-|
| `data = df_seen`| Specifies the dataset to be used in the experiment, in this case, df_seen. |
| `target = 'cnt'`| Specifies the target variable, which is 'cnt' (count of bike rentals), to be predicted in the experiment. |
| `train_size = 0.8`| Determines the size of the training data, set to 80% of the total data, for building machine learning models. |
| `preprocess = False`| With the use of a custom preprocessing pipeline, this indicates that the prprocessing step will not be executed by Pycaret. |
| `session_id = RANDOM_SEED`| Sets the session ID to ensure reproducibility of the experiment, using the predefined variable RANDOM_SEED. |
| `fold = K_FOLDS`| Determines the number of folds for crosvalidation during model training, using the previously defined K_FOLDS. |
| `n_jobs = 4`| Specifies the number of workers to be used for parallel tasks during the model training process. |
| `custom_pipeline = base_pipe`| Utilizes the custom pipeline defined earlier (base_pipe) as part of the experiment. |

In this data exploration, each session has a unique identification number, and the main goal is to predict the number of bicycles rented ('cnt') with a regression type. The initial data has dimensions (9732, 11), and after transformation, it remains at the same size. However, the training set has 7785 rows, while the test set has 1947 rows. The dataset consists of 9 numeric features and 1 date feature. This configuration creates an experimental framework to analyze rental bike trends based on these features.

The base pipeline will go through preprocessing including outlier handling, then feature creation by extracting 'dteday' features, then feature selection by dropping columns that are not used and can cause redundancy or multicolinearity. An encoding stage will be added to some distance and scale based models.

**The Results Of Feature Engineering That Goes Through The Base Pipeline:**
"""

get_config('X_train_transformed')

"""**Setup Base Model and Log Models with Target Transformers.**"""

from lightgbm import LGBMRegressor
from sklearn.linear_model import BayesianRidge

base_models = {
    'dt': DecisionTreeRegressor(),
    'rf': RandomForestRegressor(),
    'et': ExtraTreeRegressor(),
    'lgbm': LGBMRegressor(),
    'xgboost': XGBRegressor(),
    'catboost': CatBoostRegressor(verbose=False),
    'knn': KNeighborsRegressor(),
    'bayesian_ridge': Pipeline(steps=
            [
                ('preprocess', encoder),
                ('model', BayesianRidge())
            ]),
    'lr': Pipeline(steps=
            [
                ('preprocess', encoder),
                ('model', LinearRegression())
            ]),
    'ridge': Pipeline(steps=
            [
                ('preprocess', encoder),
                ('model', Ridge())
            ]),
    'lasso': Pipeline(steps=
            [
                ('preprocess', encoder),
                ('model', Lasso())
            ]),
    'svr': Pipeline(steps=
            [
                ('preprocess', encoder),
                ('model', SVR())
            ]),
}

log_models = {}
for key, val in base_models.items():
    model = create_model(estimator=TransformedTargetRegressor(
                        regressor=val,
                        func=np.log1p,
                        inverse_func=np.expm1
                        ), verbose=False)
    log_models[key] = model

tuned_models = {}

"""### **Models Description**

-----------------------

### **Tree Regression**
Tree regression models build tree-like structures to make predictions by recursively splitting the dataset based on feature values. These models are highly interpretable and capture non-linear relationships in the data.

| Experiment | Models | Description |
|-|-|-|
| Tree Regression Models | Decision Tree Regressor (dt) | `Decision tree` is a tree-shaped model that represents a set of decisions based on features. It recursively splits the dataset into subsets based on the most significant attribute, resulting in a tree structure. |
| | Random Forest Regressor (rf) | `Random Forest` is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mean prediction of the individual trees for regression tasks. It helps reduce overfitting and improves accuracy. |
| | Extra Tree Regressor (et) | `Extra Trees`, or Extremely Randomized Trees, is an ensemble method similar to Random Forest. However, it goes a step further by using random thresholds for each feature rather than finding the best one, making it even less prone to overfitting. |

-----------------------

### **Linear Regression**
Linear regression models assume a linear relationship between the features and the target variable. These models are simple, interpretable, and perform well for linearly separable datasets.

| Experiment | Models | Description |
|-|-|-|
| Linear Regression Models | Linear Regression (lr) | `Linear Regression` models the relationship between the dependent variable and one or more independent variables by fitting a linear equation to the observed data. It assumes a linear relationship between the variables. |
| | Ridge Regression (ridge) | `Ridge Regression` is a linear regression variant that includes a regularization term (L2 norm) to prevent overfitting. It is particularly useful when multicollinearity is present in the dataset. |
| | Lasso Regression (lasso) | `Lasso Regression` is similar to Ridge Regression but uses the L1 norm for regularization. It not only helps prevent overfitting but also performs feature selection by driving some coefficients to exactly zero. |

-----------------------

### **Gradient-Based Regression**
Gradient-based regression models iteratively optimize a loss function to make accurate predictions. These models are robust and effective for capturing complex, non-linear relationships.

| Experiment | Models | Description |
|-|-|-|
| Gradient-Based Regression Models | XGBoost Regressor (xgboost) | `XGBoost` is an optimized gradient boosting algorithm that is widely used for regression tasks. It sequentially builds a series of decision trees, correcting errors of the previous ones, and combines their predictions. |
| | CatBoost Regressor (catboost) | `CatBoost` is another gradient boosting-based ensemble method, designed to handle categorical features efficiently. It optimizes performance and is known for its ease of use and high predictive accuracy. |
| | LightGBM Regressor (lgbm) | `LightGBM` is a gradient boosting framework optimized for speed and memory efficiency. It uses histogram-based algorithms for faster training and is suitable for large datasets with numerous features. |

-----------------------

### **Other Regressions**
This category includes regression models that do not fall under the above classifications, offering unique approaches to prediction.

| Experiment | Models | Description |
|-|-|-|
| Other Regression Models | K-Nearest Neighbors Regressor (knn) | `K-Nearest Neighbors` is a non-parametric algorithm used for regression tasks. It predicts the output based on the average of the 'k' nearest data points in the feature space. |
| | Bayesian Ridge Regression (bayesian_ridge) | `Bayesian Ridge Regression` incorporates Bayesian inference to estimate uncertainty in the model's coefficients, providing probabilistic predictions. |
| | Support Vector Regression (svr) | `Support Vector Regression` uses support vectors to perform linear regression. It aims to find a hyperplane that best represents the data while minimizing deviations from the actual values. Kernel functions can be employed for non-linear relationships. |

-----------------------

### **Log Models (Transformed Target Regressor)**
For all models above, the `TransformedTargetRegressor` is used with a logarithmic transformation (`np.log1p`) on the target to address skewness. The original values are then restored using the exponential function (`np.expm1`) to obtain predictions that can be interpreted in the original scale. This approach improves model performance on skewed target distributions.

-----------------------

### **First Experiment: Benchmark Base Model without Tranformed Target (Imbalance Target Distribution)**
"""

first = compare_models(base_models.values())

"""### `Insight`

Based on the benchmark results:

- **CatBoost Regressor** demonstrates the best performance among all models, with the lowest MAE (25.74), MAPE (46.45%), and a high R² (0.95). This indicates that CatBoost has a strong ability to predict accurately. However, its training time (2.7 sec) is relatively higher than gradient-based models like LightGBM and XGBoost.

- **Light Gradient Boosting Machine** also exhibits solid performance, with MAE (26.90) and MAPE (43.17%) slightly higher than CatBoost. Its strength lies in its extremely fast training time (0.19 sec), making it an excellent choice for scenarios requiring quick training.

- **Extreme Gradient Boosting** performs competitively, with an MAE (27.39) and MAPE (45.44%) close to those of LightGBM. While its training time (0.21 sec) is marginally higher, the model maintains high accuracy with an R² (0.94), making it a reliable choice for complex datasets.

### **Second Experiment: Benchmark Model with Tranformed Target (Balanced Target Distribution)**
"""

second = compare_models(log_models.values())

"""### `Insight`

Based on the benchmark results using transformed targets:

- **CatBoost Regressor** achieves the best performance with the lowest MAE (23.81), MAPE (23.36%), and a high R² (0.95). This indicates its exceptional ability to handle transformed targets effectively, providing robust predictions with a training time of 2.51 seconds.

- **Extreme Gradient Boosting (XGBoost)** also demonstrates strong performance with an MAE (26.44) and MAPE (25.54%) close to CatBoost. Its R² (0.94) indicates a high level of variance explanation, and its training time (0.21 seconds) makes it an efficient alternative to CatBoost for similar tasks.

- **Light Gradient Boosting Machine (LightGBM)** follows closely, with an MAE (27.37) and MAPE (25.65%), coupled with a high R² (0.94). Its extremely fast training time (0.20 seconds) highlights its suitability for quick model training while maintaining competitive performance.

### **Conclusion**

Based on the analysis and benchmarking results, it is evident that regression models, particularly non-linear and gradient-based regression models, perform well on the bike-sharing dataset. These models effectively handle the complexity of predicting bike rental demand based on factors like weather, time, and other contextual variables.

1. **Strong Predictive Power:** Models like **CatBoost**, **XGBoost**, and **LightGBM** demonstrate high R² values (above 0.93) with low errors (MAE and MAPE), indicating their ability to explain the variability in bike rental demand and make precise predictions.
   
2. **Gradient-Boosted Models Excel:** **CatBoost** leads in predictive accuracy, making it the most robust model for this problem. **XGBoost** and **LightGBM** follow closely, offering a strong balance between accuracy and training efficiency.
   
3. **Efficiency in Model Training:** Models like **LightGBM** are exceptionally fast during training (0.20 seconds) while maintaining competitive performance, making them ideal for real-time applications or iterative model tuning.

4. **Linear Models Lag Behind:** Models such as **Linear Regression**, **Bayesian Ridge**, and **Ridge Regression** struggled to capture the dataset's non-linear relationships, evidenced by relatively lower R² values (~0.71) and higher errors.

5. **Suboptimal Models:** Simpler models like **Decision Tree** and **K-Nearest Neighbors** performed moderately, while **Lasso Regression** showed the weakest results, suggesting it is not suitable for this dataset.

**Next Steps: Focus on hyperparameter tuning for these top-performing models to further enhance their predictive accuracy and optimize performance.**

### **Hyperparameter tuning of Best Model**

**1. CatBoost Regressor**

We chose this model for tuning because it consistently performed best in the first and second experiments.
"""

hyperparam_catboost = {
    'regressor__depth': [7, 8, 9],
    'regressor__learning_rate': [0.06, 0.07, 0.08],
    'regressor__iterations': [1400, 1500, 1600]
}

tuned_catboost = tune_model(
    estimator=log_models['catboost'],
    optimize='mape',
    custom_grid=hyperparam_catboost,
    search_library='scikit-learn',
    search_algorithm='grid',
    return_train_score=True,
    verbose=True
)

catboost_tuned = pull()

"""`Insight`

- Consistently low MAE (12.28), MSE (432.90), RMSE (20.80), and high R² (0.9869) on the training set indicate excellent fit and stability.  
- Validation metrics show higher MAE (22.44), RMSE (37.88), and MAPE (28.10%) with a drop in R² (0.9565), highlighting overfitting.  
- Variability in validation metrics (e.g., RMSE std = 2.57) suggests some data distribution differences across folds.  
- Low training metric standard deviation indicates stable performance across training folds.

**Best Hyperparameters For CatBoost Regressor:**

The hyperparameter tuning process to determine the best combination to optimize the Mean Absolute Percentage Error (MAPE) is as follows:
"""

tuned_catboost.regressor.get_params()

catboost_tuned_params = {'iterations': 1500,
                         'learning_rate': 0.07,
                         'depth': 8}

tuned_models['catboost'] = create_model(log_models['catboost'], verbose=False)
tuned_models['catboost'].regressor.set_params(**catboost_tuned_params)

"""The tuned parameters for CatBoost, specified as `catboost_tuned_params`, highlight a configuration optimized for performance. These include a learning rate of 0.06, a tree depth of 8, and 1400 iterations, indicating the number of boosting rounds during training to balance accuracy and generalization.

**Comparison Before and After Tuning For CatBoost**
"""

print('Before Tuning:')
create_model(log_models['catboost'])

print('After Tuning:')
create_model(tuned_models['catboost'])

"""`Insight`

- After tuning, all evaluation metrics show improvement. MAE, RMSE, MSE, RMSLE, and MAPE decrease, indicating enhanced accuracy and better predictive performance.  
- Mean MAE improves by approximately 1.42 (23.81 to 22.39), showing better precision in predictions.  
- The R² value increases slightly from 0.9533 to 0.9571, highlighting improved variance explanation and generalization.  
- Variability (std) in metrics remains consistent, reflecting stable performance across folds.

**2. Extreme Gradient Boosting**

This model was chosen because it has a relatively shorter training time with very good results compared to other models.
"""

hyperparam_xgboost = {
    'regressor__max_depth': [4, 5, 6],
    'regressor__learning_rate': [0.04, 0.05, 0.06],
    'regressor__n_estimators': [1200, 1300, 1400],
    'regressor__min_child_weight': [0,1,2],
    'regressor__subsample': [0.7, 0.8, 0.9]
}

tuned_xgboost = tune_model(
    estimator=log_models['xgboost'],
    optimize='mape',
    custom_grid=hyperparam_xgboost,
    search_library='scikit-learn',
    search_algorithm='grid',
    return_train_score=True,
    verbose=True
)

xgboost_tuned = pull()

"""`Insight`

- Low MAE (14.10), MSE (567.61), RMSE (23.82), and high R² (0.9829) on the training set indicate strong model performance and consistency.  
- Validation metrics, including higher MAE (23.82), RMSE (39.94), and MAPE (23.62%), alongside a high R² (0.9517), suggest some overfitting but acceptable generalization.  
- Variability across validation folds, with standard deviations in MAE (0.92) and RMSE (2.35), highlights potential differences in data distribution or subsets.  
- Minimal standard deviation in training metrics reflects the model's stability during training.

**Best Hyperparameters For Extreme Gradient Boosting:**

The hyperparameter tuning process to determine the best combination to optimize the Mean Absolute Percentage Error (MAPE) is as follows:
"""

tuned_xgboost.regressor.get_params()

xgboost_tuned_params = {'learning_rate': 0.05,
                         'max_depth': 5,
                         'min_child_weight': 0,
                         'n_estimators': 1300,
                         'subsample': 0.9}

tuned_models['xgboost'] = create_model(log_models['xgboost'], verbose=False)
tuned_models['xgboost'].regressor.set_params(**xgboost_tuned_params)

"""The tuned parameters for XGBoost suggest a well-balanced configuration for optimizing performance. Key hyperparameters include a learning rate of 0.05, a tree depth of 5, and 1300 estimators, which indicate the model's ability to learn effectively while maintaining a reasonable complexity for the decision trees. The choice of a `min_child_weight` of 2 helps control overfitting by requiring a minimum amount of data in each leaf. Additionally, the `subsample` value of 0.9 ensures that 90% of the training data is used in each boosting round, helping the model generalize well without overfitting. Overall, these parameters reflect a robust model setup aimed at achieving a balance between bias and variance.

**Comparison Before and After Tuning For Extreme Gradient Boosting**
"""

print('Before Tuning:')
create_model(log_models['xgboost'])

print('After Tuning:')
create_model(tuned_models['xgboost'])

"""`Insight`

- After tuning, there is a noticeable improvement in the model's performance across all metrics. The mean MAE decreases from 26.44 to 23.86, indicating improved prediction accuracy. Similarly, MSE and RMSE values decrease, suggesting better error minimization and a reduction in prediction discrepancies.

- R² improves slightly from 0.9407 to 0.9517, indicating a better fit to the data and a higher proportion of variance explained by the model.

- MAPE decreases from 0.2554 to 0.2366, showing enhanced model efficiency in predicting relative errors.

- RMSLE remains relatively stable, but the reduction in standard deviation for both MAE and RMSE reflects increased model stability after tuning.

- Overall, the tuning process significantly enhanced the model's generalization and stability, achieving better accuracy while maintaining reliability across folds.

**3. Light Gradient Boosting Machine (LightGBM)**

We chose LightGBM because this model consistently performed well in the experiments, achieving high accuracy and competitive metrics. By tuning its hyperparameters, we aim to further improve its predictive performance.
"""

hyperparam_lgbm = {
    'regressor__n_estimators': [100, 200, 500],
    'regressor__learning_rate': [0.01, 0.05, 0.1],
    'regressor__max_depth': [-1, 5, 10],
    'regressor__num_leaves': [31, 50, 100],
    'regressor__min_child_samples': [10, 20, 30]
}

tuned_lgbm = tune_model(
    estimator=log_models['lgbm'],
    optimize='mape',
    custom_grid=hyperparam_lgbm,
    search_library='scikit-learn',
    search_algorithm='grid',
    return_train_score=True,
    verbose=True
)

lgbm_tuned = pull()

"""`Insight`

- Low MAE, MSE, RMSE, and high R² on the training set indicate strong model performance and consistency.
- Validation performance shows higher MAE, RMSE, and MAPE, with R² remaining high, suggesting slight overfitting and reduced generalization.
- Variability in validation metrics across folds suggests potential differences in data distribution or model sensitivity to certain subsets.
- Minimal standard deviation in training metrics reflects stability and reliability in the model's training performance, but the increased validation MAPE hints at some potential for improvement in generalization.

**Best Hyperparameters For Light GBM:**

The hyperparameter tuning process to determine the best combination to optimize the Mean Absolute Percentage Error (MAPE) is as follows:
"""

tuned_lgbm.get_params()

lgbm_model = tuned_lgbm.regressor

lgbm_tuned_params = {
    'n_estimators': 500,
    'learning_rate': 0.05,
    'max_depth': -1,
    'num_leaves': 50,
    'min_child_samples': 20
}


tuned_models['lgbm'] = create_model(log_models['lgbm'], verbose=False)
lgbm_model.set_params(**lgbm_tuned_params)

"""The tuned parameters for the LightGBM model suggest a well-optimized configuration for achieving both high performance and efficient training. Key hyperparameters include a learning rate of 0.05, a maximum depth of -1 (allowing unlimited tree depth), and 500 estimators, which indicate the model's ability to learn complex patterns while avoiding excessive complexity in individual trees. The use of 50 leaves in each tree allows for sufficient flexibility in capturing non-linear relationships without introducing too much variance. The min_child_samples value of 20 ensures that each leaf contains enough samples to avoid overfitting, while the subsample rate of 1.0 leverages the full dataset to maximize model performance. The min_split_gain of 0.0 allows for splits even with minimal improvement, ensuring that the model explores all potential splits in the data. Overall, these hyperparameters reflect a robust setup for achieving a balance between model accuracy and generalization.

**Comparison Before and After Tuning For Light GBM Regression**
"""

tuned_models['lgbm'] = lgbm_model

print('Before Tuning:')
create_model(log_models['lgbm'])

print('After Tuning:')
create_model(tuned_models['lgbm'])

"""`Insight`

After tuning, there is a noticeable improvement in the model's performance across all key metrics.

- The mean MAE decreases from 27.37 to 24.40, indicating a significant enhancement in prediction accuracy.

- MSE and RMSE also show considerable reductions, from 2038.47 to 1681.87 and from 45.08 to 40.95, respectively, suggesting better error minimization and a decrease in large prediction discrepancies.

- R² improves slightly from 0.9385 to 0.9492, reflecting a better fit to the data and an increase in the proportion of variance explained by the model.

- MAPE decreases from 0.2565 to 0.2377, demonstrating enhanced model efficiency in predicting relative errors with greater consistency.

- RMSLE remains stable, but the reduced standard deviation in both MAE and RMSE indicates increased stability and reliability across folds.

Overall, the tuning process has significantly enhanced the model's accuracy and generalization ability, improving its performance while ensuring stability across different data subsets.

### **Model Selection**

After tuning the best 3 models, we compared the 3 models to consider which model to use for the final model.
"""

print('Before Tuning:')
compare_models([log_models['catboost'], log_models['xgboost'], log_models['lgbm']])

print('After Tuning:')
compare_models(tuned_models.values())

"""**Summary**

| Model | MAE | MAPE | R2 | TT (Sec) |
|-|-|-|-|-|
| CatBoost Regressor | 22.75 (+0.00) | 22.81% (+0.00) | 0.95 (+0.00) | 0.73 (+0.00) |
| Extreme Gradient Boosting	 | 24.25 (+1.52) | 23.63% (+0.82) | 0.95 (+0.00) | 0.19 (-0.54) |
| Support Vector Regression	 | 29.93 (+5.68) | 28.15% (+4.52) | 0.92 (-0.03) | 5.46 (+5.27) |

`Insight`

- From the three tuned models, CatBoost has the most accurate predictions of the other models based on all evaluation metrics.

- CatBoost has low error with MAE (22.82) and MAPE (23.32%). Apart from that, it has a very high R2 value (0.95) which shows that around 95% of the variation in the data can be explained by this model and requires a training time of 1.11 Sec.

- Extreme Gradient Boosting is also very good at predicting models with error values ​​that are not much different from the Catboost model with a difference in MAE (+1.52) and MAPE (+0.82%) but with a training time (0.19 sec) shorter about 4x faster than catboost.

- Meanwhile, although Support Vector Regression has significantly improved after tuning, it is still not better than Catboost and Extreme Gradient Boosting models with MAE (29.93) and MAPE (28.15%). In addition, this model requires a relatively longer training time than the others of about 5 seconds so this model is not recommended.

Based on this analysis, CatBoost Regressor may be the best choice because it provides a good balance between high accuracy and reasonable training time. However, if training speed is a priority, Extreme Gradient Boosting can also be considered a good alternative. Considering business problems, CatBoost is the most appropriate choice because it predicts demand more accurately, thus avoiding losses and potential profit loss.

**Comparison K-Fold Cross-Validation**
"""

import time

cross_val = pd.DataFrame()

for k in range(1, 11):
    start_time = time.time()
    if k == 1:
        model = create_model(tuned_models['catboost'], verbose=False, cross_validation=False)
        results = pd.DataFrame(pull().loc['Test']).T
    else:
        model = create_model(tuned_models['catboost'], verbose=False, fold=k)
        results = pd.DataFrame(pull().loc['Mean']).T

    results.insert(0, 'K-Fold', k)
    elapsed_time = time.time() - start_time
    results['Time (Sec)'] = elapsed_time

    # Use pd.concat() instead of append()
    cross_val = pd.concat([cross_val, results], ignore_index=True)

cross_val

sns.set_style('darkgrid')

fig, axs = plt.subplots(2, 3, figsize =(20,8))
axs = axs.flatten()

metric_eval = ['MAE', 'RMSE', 'RMSLE', 'MAPE', 'R2', 'Time (Sec)']
for i in range(len(metric_eval)):
    sns.lineplot(data=cross_val, x='K-Fold', y=metric_eval[i], ax=axs[i], marker='o')

fig.tight_layout()
plt.suptitle('K-Fold Cross-Validation Results', y=1.02, fontsize=16)
plt.show()

"""`Insight`

- The errors in MAE, RMSE, RMSLE, MAPE with cross-validation tend to decrease as the k-fold value increases (2-10). This indicates that with higher k-fold, the model tends to provide more accurate predictions.

- The R2 (Coefficient of Determination) value also tends to increase from k=2 to k=10 (with cross-validation), indicating that the model is getting better at explaining the variability in the target data.

- The execution time increases as the k-fold value increases. This is reasonable because the higher the k-fold value, the more iterations are needed for cross-validation, which requires longer computation time.

- In this case, without cross validation (k=1) is the best result with MAE (22.27) MAPE (21.88%) with the shortest time (3.21 seconds) and R2 (0.96).

- There is no significant change between without using cross validation (k=1) or with cross validation from k=2 to k=10. This indicates stable model performance without samples or with different sample distributions.

**Features Selection using Wrapper Method**

Before finalizing the model, we perform feature selection by using a wrapper method with a sequential search forward selection strategy. The wrapper uses the search strategy to search through the space of possible feature subsets and evaluates each subset by the quality of performance on the ML algorithm. Practically, any combination of search strategy and algorithm can be used as a wrapper.

The purpose of feature selection is to select the most relevant and significant subset of features from the set of features available in the data with the aim of improving the accuracy of the model and increasing computational efficiency which can cut down time.
"""

from mlxtend.feature_selection import SequentialFeatureSelector

forward = SequentialFeatureSelector(
    estimator=tuned_models['catboost'],
    k_features='best',
    forward=True,
    verbose=0,
    scoring='neg_mean_absolute_percentage_error',
    cv = 5,
)

forward.fit(get_config('X_train_transformed'), np.log1p(get_config('y_train_transformed')))

best_feature = pd.DataFrame.from_dict(forward.get_metric_dict()).T

display(best_feature.sort_values(by='avg_score', ascending=False))
selected_feature = list(best_feature.loc[10, 'feature_names'])
selected_feature

"""`Insight`

- Models were evaluated with various combinations of the number of features, ranging from a single feature to all available features.

- The more features included, the better the cross-validation scores (cv_scores) generally are, but this improvement may decrease as more features are added.

- The best average cross validation score result is -0.065673 on the 11th index (total features selected) with no one feature is drop.
"""

final_model = tuned_models['catboost']
final_model

"""### **Evaluate Final Model**

**Learning Curve**
"""

from yellowbrick.model_selection import LearningCurve
from sklearn.metrics import mean_absolute_percentage_error, make_scorer

curve = LearningCurve(final_model, scoring=make_scorer(mean_absolute_percentage_error))
curve.fit(get_config('X'), get_config('y'))
curve.show()

"""`Insight`

- The learning curve on the CatBoostRegressor model shows that this model has high scores (low mape) on both training and validation data.

- Although the score on the training data is slightly higher than the score on the validation data, the validation score tends to get closer to the score on the training data as the training process progresses. This indicates that the model has good performance and is not underfitting or overfitting.

- The stability of scores on training data and validation data during the training process indicates that the model has understood most of the patterns in the data. This stability indicates that the model is able to generalize well to data that has never been seen before.

- The closer the validation data score is to the training data score as the training process increases, it shows that adding more training data can improve model performance. This illustrates that the model can benefit from more training data to improve its ability to understand complex patterns.

- Overall, the learning curve results show that the CatBoostRegressor model has learned well and has good generalization capabilities on never-before-seen data.

**Residual Analysis**
"""

plot_model(estimator=final_model, plot='residuals')

"""`Insight`

- The residuals approaching the zero line indicate that the model predictions are relatively accurate and the prediction error is small.

- The R-Squared line in the graph shows high R-Squared values ​​in both training and test data, indicating good predictive ability.

- A near-normal distribution of residuals, especially around zero, is positive for linear regression and indicates unsystematic prediction error.

- Overall, the graph confirms that the CatBoost Regressor model has good predictive ability, low error, and a near-normal distribution of residuals, strengthening confidence in the reliability and consistency of the model's predictions.

**Data Validation Prediction**
"""

validation_prediction = predict_model(final_model)
validation_prediction['error'] = validation_prediction['prediction_label'] - validation_prediction['cnt']
validation_prediction['abs_error'] = validation_prediction['error'].abs()
validation_prediction['square_error'] = validation_prediction['error']**2
validation_prediction['percentage_error'] = validation_prediction['error'].abs()/validation_prediction['cnt']
df_residual = validation_prediction[['cnt', 'prediction_label', 'error', 'abs_error', 'square_error', 'percentage_error']]
df_residual.head()

"""`Insight`

From the results above, The final model prediction results against the validation data are not much different from the model results after the tuning process. even the prediction results against the validation data are slightly better with MAPE decreasing from 22.81% to 22.26%, R2 from 0.9557 to 0.9573. only MAE increased from 22.7507 to 23.0664 but not significantly.
"""

sns.set_style('darkgrid')

plt.figure(figsize=(15,6))
sns.scatterplot(data=validation_prediction, y='prediction_label', x='cnt', label='Prediction')
sns.regplot(data=validation_prediction, x='cnt', y='cnt', scatter=False, line_kws = {'linewidth': 2, 'linestyle':'--'}, color='black', label='Best Fit')

plt.title('Actual vs Prediction: Validation Data', fontsize=16)
plt.xlabel('Actual')
plt.ylabel('Prediction')
plt.legend()
plt.show()

"""`Insight`

- The prediction is generally quite accurate, as seen from the data points that are close to the red line, representing the supposed value. However, there are still some data points that are far from the red line, indicating a degree of error in the prediction model.

- Some data points that are far from the red line indicate that the model has difficulty in predicting correctly in certain cases.

- Evaluation of metrics, such as Mean Absolute Percentage Error (MAPE), shows that the average error at these points is up to 22% of the actual value. Although there is uncertainty, the error is still within the acceptable range.

- Overall, the model seems to provide reasonably good predictions with acceptable error levels.
"""

fig, axs = plt.subplots(1, 2, figsize=(20,6))
sns.distplot(validation_prediction['cnt'], ax=axs[0])
sns.distplot(validation_prediction['prediction_label'], ax=axs[1])
axs[0].set_title('Actual Data Distribution')
axs[1].set_title('Prediction Data Distribution')
plt.show()

"""`Insight`

- Both the actual and predicted data distributions show a similar shape with most of the data concentrated around low values. However, there is a noticeable difference in the peaks and tails of the distribution in the prediction data graph.

- In the predicted distribution, it can be seen that the model tends to produce more dispersed values compared to the actual data. This is evident from the longer tails of the distribution and the lack of sharp peaks as in the actual data.

- Although the model is able to capture the general shape of the actual data distribution, it seems to be less accurate in predicting the frequency of lower values.

- There are indications that the model may tend to overestimate for smaller values, as seen from the difference in peak height between the actual and predicted data.

- A prediction distribution that shows greater variability could indicate that the model may be less precise or too flexible in predicting extreme values.

- In conclusion, while the model can capture the general trend, it should be noted that there is a tendency to overestimate small values and greater variability in extreme values in the predictions.
"""

bins = [0, 50, 100, 200, 300, 400, 500, 600, float('inf')]
labels = ['0-50', '50-100', '101-200', '201-300', '301-400', '401-500', '501-600', '>600']

df_residual['cnt_bins'] = pd.cut(df_residual['cnt'], bins=bins, labels=labels, right=False)

residual_val = df_residual.groupby('cnt_bins').mean().reset_index()
residual_val

fig, axs = plt.subplots(1, 2, figsize=(15, 6))

mae_bar=sns.barplot(x='cnt_bins', y='abs_error', data=residual_val, hue='cnt_bins', palette=color, ax=axs[0])
for idx, bar in enumerate(mae_bar.patches):
        x_value = bar.get_x() + bar.get_width() / 2
        y_value = bar.get_height()
        label = "{:.0f}".format(bar.get_height())
        axs[0].text(x_value, y_value, label, ha='center', va='bottom')

mape_bar=sns.barplot(x='cnt_bins', y=residual_val['percentage_error']*100, data=residual_val, hue='cnt_bins', palette=color, ax=axs[1])
for idx, bar in enumerate(mape_bar.patches):
        x_value = bar.get_x() + bar.get_width() / 2
        y_value = bar.get_height()
        label = "{:.2f}%".format(bar.get_height())
        axs[1].text(x_value, y_value, label, ha='center', va='bottom')

axs[0].set_title('Mean Absolute Error')
axs[0].set_xlabel('cnt_bins')
axs[0].set_ylabel('Absolute Error')
axs[1].set_title('Mean Absolute Percentage Error')
axs[1].set_xlabel('cnt_bins')
axs[1].set_ylabel('Percentage Error (%)')

plt.tight_layout()
plt.show()

"""`Insight`

- In the value range of 1 to 50 the model is less reliable as there is a sizable average error of about 43% based on the MAPE, therefore, we can see the MAE value is still relatively small at about 5 points.

- The model performs relatively well when predicting the 50-100 and 101-200 value ranges, with the average MAPE errors against the original values being around 23%, 16%, and 13% respectively which is still relatively acceptable. The average MAE errors are also at an acceptable level, at around 17, 23, and 31 points respectively.

- Meanwhile, although there are relatively large average MAE errors for the prediction value ranges of 301-400, 401-500, 501-600, >600, which are about 34, 45, 53, and 47. However, based on the average percentage MAPE errors, the data with these large ranges are the most reliable, this is because the average errors are within 7% to 10% of the actual data.

**Feature Importance**
"""

interpret_model(final_model, plot='summary')

"""`Insight`

- Features that have a big impact on the prediction results include hr, temp, dteday_year, dteday_weekday, season, and humidity.

- High values of the hr feature have a large positive impact on the prediction results, but low values of the hr feature have a larger negative impact on the prediction results, as well as the temp and season features.

- High values of windspeed have a negative impact but this negative impact is less than the high values of hum and weathersit which have a greater negative impact on the prediction results.

- 2011 has a negative impact while 2012 has a positive impact on the prediction results.

- The low value of month tends to have a negative impact on the prediction results, while the low value of holiday feature does not have any impact on the prediction results.

### Finalize Model
"""

catboost_final_model = finalize_model(final_model)
catboost_final_model

"""The following is the work process end to end on the model based on the pipeline above:

1. `Outlier handling`: Resolve outliers in certain features (weathersit, hum, windspeed).

2. `Date Extraction (feature creation)`: create new features such as 'day', 'month', 'year', and 'weekday' extracted from the 'dteday' column.

3. `Feature selection before modeling (filter method)`: feature selection by removing features that may cause multicollinearity or redundancy, in this case the features 'atemp', 'casual', 'registered'.

4. `Modeling-Target Transformation`: since the target is significantly skewed, to improve the model performance results, the target needs to be transformed using a logarithmic function during training and transformed back using an exponential function during prediction.

5. `Modeling-Feature Selection during modeling (wrapper method)`: based on feature selection with squential feature selection and rfe some features are not significant to model performance so they need to be removed. to improve model performance and more efficient computation. in this case the features 'hum' and 'windspeed' are removed.

6. `Modeling-Prediction with CatBoost Regressor`: Using CatBoost Regressor to predict the target.

CatBoost employs the Boosting technique in its training process. Boosting is an ensemble learning method where weak models are sequentially adjusted to the data, and each subsequent model aims to correct the errors produced by the previous model. This process allows CatBoost to generate a strong and accurate model by focusing on addressing the aggregate errors of the preceding models. With this approach, CatBoost can effectively handle categorical data and deliver reliable prediction results across various machine learning tasks, such as classification and regression.

**Predicting Unseen Data (Data Testing)**
"""

predictions_unseen = predict_model(catboost_final_model, data=df_unseen)

predictions_unseen['error'] = predictions_unseen['prediction_label'] - predictions_unseen['cnt']
predictions_unseen['abs_error'] = predictions_unseen['error'].abs()
predictions_unseen['square_error'] = predictions_unseen['error']**2
predictions_unseen['percentage_error'] = predictions_unseen['error'].abs()/predictions_unseen['cnt']
predictions_unseen = predictions_unseen[['cnt', 'prediction_label', 'error', 'abs_error', 'square_error', 'percentage_error']]
predictions_unseen.head()

"""`Insight`

- Overall, the CatBoost Regressor model performed well on the test unseen data.

- The high R2 value (0.9552) indicates that the model can explain most of the variation in the target.

- The absolute and relative errors (MAE: 21.82 and MAPE: 22.81%) are also low, indicating that the model predictions are relatively accurate.

**Advanced Analysis**

Can the model predict specific demand for casual or registered? Let's see..
"""

registered = RegressionExperiment()
registered.setup(
    target = 'registered',
    **configs
)

casual = RegressionExperiment()
casual.setup(
    target = 'casual',

    **configs
)

"""With the same setup and pipeline, I conducted experiments to test the model against other target variables, namely 'casual' and 'registered'."""

print('Registered Member Prediction:')
registered_model = registered.create_model(catboost_final_model, verbose=False)
display(registered.predict_model(registered_model, data=df_unseen).head()[['registered', 'prediction_label']])

print('Casual Member Prediction:')
casual_model = casual.create_model(catboost_final_model, verbose=False)
display(casual.predict_model(casual_model, data=df_unseen).head()[['casual', 'prediction_label']])

"""`Insight`

- By re-fitting the data, the final model can provide accurate predictions of the number of registered users with relatively low errors MAE: 17.75, MAPE: 23%. The R-squared value reached 95%, indicating that the model well understood the variation in the data.

- By re-fitting the data, the final model can provide an accurate prediction of the number of casual users with a relatively low MAE error of 8 and an R-squared value of 92%, indicating that the model can understand well the variations in the data. The high MAPE value of about 41% is due to predicting a small range of values in the 1-50 data and a value of 0, resulting in a large percentage. However, objectively we can see based on the MAE that the results are quite good.

- For example, there is an average demand of 35 per hour (based on actual data) for casual in a day, and the model predicts with an error rate of MAE: 8 on the casual prediction. This means there is an error of about 22.28% based on MAPE. Thus, the allocation of bicycles prepared is only up to 43 (45 + 8).

- Based on data from https://www.kaggle.com/code/samratp/bike-share-analysis, the average Capital Bikeshare user trip is about 18 minutes. If a user uses the Single Ride option with a duration of 15 minutes, the cost would be 1 USD + (0.05 USD x 15) = 1.75 USD per user. So for 36 users, bike sharing provider get (36 x 1.75 USD) 63 USD in an hour.  Meanwhile, the maximum average operating cost for bicycle maintenance is 0.55 USD per bicycle. With an allocation of 43 bicycles, the total operating cost is 23.65 USD (43 x 0.55 USD).

- Therefore, there is still a profit margin of about (63 - 4.95) 39.35 USD a day if the demand for the day is only 35 even though the actual data is 35 per hour. This shows that despite the inaccuracies in the prediction and the sub-optimal allocation of bicycles, there is still potential profit to be made with the error rate of the model built.

# **Conclusion and Recommendation**

**Modeling Process**

- Based on the two experiments (without target transform and with target transform) conducted, 3 best models were selected for tuning including Catboost Regressor, XGBoost Regressor, and Support Vector Regressor.

- Catboost Regressor is still the best model after tuning and is used as the final model. With tuning parameters 'iterations': 1500, 'learning_rate': 0.07, 'depth': 8 obtained average performance results with MAE: 22.75, R2: 95.57%, and MAPE: 22.81%.

- When predicting the testing data (unseen data) the performance results become better with MAE: 21.82, R2: 95.95%, MAPE: 22.81%. This indicates that the final model is good for predicting unseen data or stable for predicting other similar data.

**Model Limitation**

- The final model still has a relatively large error based on MAPE which is around 22%. This can happen because the predicted values are in a large range from 1 - 970 so the error predicts on small values, otherwise if we predict a high total number of customers, this model can be more accurate.

- The small range of values (1-50) is less reliable because the average error is quite large with 44% of the actual demand (data). However, we can justify this by looking at the MAE, where the average MAE in this range is about 5 points.

- It can be concluded that this model may be less suitable for predicting small values, but it can reliably predict better on a larger scale of values. In addition, the model may also be less accurate for the range of predicted values outside the training data (1-970).

- This model is also less reliable for predicting casual members because the MAPE is quite large 41.43% even though we can see from the MAE value which is relatively quite small, which is around 8.

**Business Impact**

- With good demand prediction, companies can optimize the allocation of resources, such as bicycles, at the right time and under the right conditions. This helps prevent imbalances between bicycle availability and user demand that can have financial implications.

- For example, if the company usually prepares the same allocation of bicycles every day and different conditions, it can cause losses because demand is lower than the allocation provided (underpredicting) which results in operating costs incurred will be greater than the benefits obtained or loss of potential profits due to demand that is higher than predicted (underpredicting) than usual due to certain conditions resulting in loss of profits that could have been obtained.

- Information from demand predicting can be used to design more targeted marketing campaigns. Companies can target special promotions or offers on days or periods with high demand, for example on weekends when demand or casual users are high.

**Recommendation**

- Since the model has limitations in predicting small values, consider fine-tuning the model specifically for the range where the demand is low (1-50). This can involve additional feature engineering, adjusting hyperparameters, or using a different modeling approach tailored to small demand values.

- Review the model evaluation metrics and ensure that the metrics used match the business objectives and demand characteristics of the bicycle. Consider using metrics such as MAPE (Mean Absolute Percentage Error) to predict a large range of values especially greater than 100 and MAE to predict a small range of values (1-50).

- In-depth analysis to understand the causes of large underestimates or overestimates. Review whether there are external factors not included in the model, changes in user behavior, or policy changes that could affect demand patterns.

# **Deployment**

**Save and Export Model**
"""

import pickle
import os

# Specify the directory
directory = "../model/"

# Ensure the directory exists
os.makedirs(directory, exist_ok=True)

# Specify the file path
filename = os.path.join(directory, "CatBoostFinalModel.pkl")  # Add a file extension for clarity

# Save the CatBoost model
with open(filename, 'wb') as file:
    pickle.dump(catboost_final_model, file)

print(f"Model saved to {filename}")

from google.colab import files

# Download the file
files.download(filename)

"""**Try to Load Model**"""

model = pickle.load(open('../model/CatBoostFinalModel.pkl', 'rb'))
model

"""Model is successfully loaded.

**Try to Predict Unseen Data**
"""

predict_model(model, df_unseen)

"""Model successfuly predicted the data."""